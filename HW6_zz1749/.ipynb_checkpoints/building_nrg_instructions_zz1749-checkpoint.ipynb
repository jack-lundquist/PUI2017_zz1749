{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# fbb skeleton notebook for PUI2017 HW6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import os\n",
    "import json\n",
    "import urllib\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import zipfile\n",
    "\n",
    "s = json.load( open(os.getenv('HOME')+'/PUI2017/fbb_matplotlibrc.json') )\n",
    "pl.rcParams.update(s)\n",
    "if os.getenv(\"PUIDATA\") is None:\n",
    "    print (\"$PUIDATA to point to set PUIdata dir\")\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys  \n",
    "\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "I am using geopanda. that is **not required** for this particular exercise, but geopanda works with geospacial data: the shape files that we get from pluto for example.\n",
    "\n",
    "PLEASE REMEMBER: seed your random functions if you need to use any, label your axes clearly, comment your code, use PEP8!\n",
    "\n",
    "\n",
    "** An interesting urban question is \"can we measure and predict energy use based on observables that are easier to be acquired\". For example the urban observatory at CUSP can monitor lights: they are a relatively easy observable. All you need is a camera, and a pipeline to process your data. But how does the light coming from a window relate to the total energy consumption? We generally postulate that light is a proxy for occupancy, and that occupancy is a good predictor of energy consumption.**\n",
    "\n",
    "** So let's test if the last link holds. If we have data on the energy consumed by a __building__ how well does that relate to the number of units in the building?**\n",
    "\n",
    "** Data on energy consumption can be found here for the city of NY https://data.cityofnewyork.us/Environment/Energy-and-Water-Data-Disclosure-for-Local-Law-84-/rgfe-8y2z  **\n",
    "\n",
    "** Either obtain the data through the API or download the csv file, and move it to $PUIDATA**\n",
    "\n",
    "** However this datasets does not have the number of units. We can find that in the [Pluto dataset](https://www1.nyc.gov/site/planning/data-maps/open-data/dwn-pluto-mappluto.page).**\n",
    "\n",
    "** Reading in the Pluto data for manhattan, which will give me the number of units ber building   Manhattan/MNMapPLUTO.shp **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/cusp/zz1749/PUIdata/Energy_and_Water_Data_Disclosure_for_Local_Law_84__2013_.csv',\n",
       " <httplib.HTTPMessage instance at 0x7fe3496b8cf8>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.urlretrieve('https://data.cityofnewyork.us/api/views/rgfe-8y2z/rows.csv?accessType=DOWNLOAD',os.getenv('PUIDATA')+\n",
    "                   '/Energy_and_Water_Data_Disclosure_for_Local_Law_84__2013_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/cusp/zz1749/PUIdata/Manhattan/mn_mappluto_16v2.zip',\n",
       " <httplib.HTTPMessage instance at 0x7fe2ea88b710>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.urlretrieve('https://www1.nyc.gov/assets/planning/download/zip/data-maps/open-data/mn_mappluto_16v2.zip',os.getenv(\"PUIDATA\") + \"/Manhattan/mn_mappluto_16v2.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shp = zipfile.ZipFile(os.getenv(\"PUIDATA\") + \"/Manhattan/mn_mappluto_16v2.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shp.extractall(os.getenv(\"PUIDATA\") + '/Manhattan/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T15:42:42.261756",
     "start_time": "2017-10-19T15:42:08.520000"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nrg = gp.GeoDataFrame.from_csv(os.getenv(\"PUIDATA\") + \"/Energy_and_Water_Data_Disclosure_for_Local_Law_84__2013_.csv\")\n",
    "# exploring the data a bit\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "sfig = scatter_matrix (nrg, s=300, figsize=(50,50), diagonal='kde')\n",
    "\n",
    "bsize = gp.GeoDataFrame.from_file(os.getenv(\"PUIDATA\") + \"/Manhattan/MNMapPLUTO.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "nrg.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "bsize.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T15:18:45.706267",
     "start_time": "2017-10-19T15:18:45.702833"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# make sure you clean up your data and throw away columns you do not need!\n",
    "bsize1 = bsize[['YearBuilt','UnitsTotal','UnitsRes','BBL']]\n",
    "nrg1 = pd.DataFrame({'DOF Number of Buildings':nrg['DOF Number of Buildings'].values,'Site EUI(kBtu/ft2)':nrg['Site EUI(kBtu/ft2)'].values,'BBL':nrg['BBL'].values,'Reported Property Floor Area (Building(s)) (ft²)':nrg['Reported Property Floor Area (Building(s)) (ft²)'].values})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed, coming up with a sensible model generally requires domain expertise. However, if the data you are investigating shows \"obvious patterns\", for example if two of the variable look like a line when plotted one against the other, then those patterns (correlations) may help you finding reasonable models for the data.\n",
    "\n",
    "Explore your data, starting with a scatter matrix. \n",
    "A scatter matrix is a plot of all variables in your data against all other variables: \n",
    "each pair of variables is a subplot in the plot matrix. The diagonal line then would be a plot of a variable against itself, which is useless, so it is usually substituted by a histogram of that variable (or sometimes a KDE, which is basically a smooth histogram)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T15:22:00.187105",
     "start_time": "2017-10-19T15:21:54.857349"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#try make a scatter plot of nrg. Few columns will plot - only those that have numerical values. \n",
    "#Pandas will ignore the other ones\n",
    "\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "scatter_matrix (nrg1, s=300, figsize=(16, 16));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Figure 1: scatter matrix of all numerical values in the files. ... comments on what you see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "This kind of plot shows correlations between variables, but it will also show me what can and cannot be plotted trivially in my dataset. Here only a few columns can be plotted: those that contain only numbers (and possibly NaN's), but most columns contain rows that cannot be converted to float (e.g. entries like 'See Primary BBL' in several rows for the energy dataframe 'Site EUI(kBtu/ft2) ' column) , so Pandas refuses to plot them, cause it does not know what to do with those entries. The columns I am interested in are in fact u'Site EUI(kBtu/ft2)' which is a measure of the energy consumed PER SQ FOOT by a building, and then the building area: for eg. u'Reported Property Floor Area (Building(s)) (ft²)'. Neither gets plotted: I need to remove all values that cannot convert to float in order to use the columns and convert them to float arrays and do math with them.\n",
    "\n",
    "You can use pd.to_numeric() which is a function that transforms values to float (when possible). The default behavior of this function is to throw an error if it encounters a value that it cannot convert. That behavior can be modified with the \"error\" keyword, buy setting it to \"coerce\". Please look at the function documentation to understand the syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_numeric(df):\n",
    "    for i in range(len(df.columns)):\n",
    "        df.iloc[:,i] = pd.to_numeric(df.iloc[:,i],errors='coerce')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_numeric(nrg1)\n",
    "to_numeric(bsize1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# use pd.to_numeric to convert strings to numeric values, \n",
    "##check that your conversion worked: e.g.\n",
    "print (nrg1['Site EUI(kBtu/ft2)'].astype(float))\n",
    "#[...] do this for all columns you care about in both datasets. \n",
    "#Nan's are ok, but you must not get an error when you try the conversion\n",
    "#the Pluto data is much better at not using spurious entries for numerical value columns. \n",
    "#but check you can use the columns you want\n",
    "bsize1.BBL.astype(float)\n",
    "#this should not return an error\n",
    "#notice I can use the attribute notation to refer to columns in bsize, \n",
    "#cause the column names do not have spaces!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T15:45:44.349333",
     "start_time": "2017-10-19T15:45:44.345266"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop everything you do not need to lighten the memory load on your machine! this is important!! \n",
    "#this file has a lot of columnsm most of them you will not need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T15:46:50.865485",
     "start_time": "2017-10-19T15:46:50.858740"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#How many missing values?\n",
    "indx = np.isnan(nrg1['Site EUI(kBtu/ft2)']).sum()\n",
    "print (\"invalid entries changed to NaN %d\"%sum(indx))\n",
    "#do it for however many columns you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "** MERGE THE DATASETS**\n",
    "look at the syntax for pandas.merge - this will be incredibly useful to you in all future data problem where you use Pandas and data aggregation is really at the heart of urban science!\n",
    "\n",
    "TO DO IT WE NEED A COMMON COLUMN: the building id, BBL is in both files. However the name of this variable (column) in the Energy dataset is 'NYC Borough, Block, and Lot (BBL)'. \n",
    "You can rename the column, create a whole new column 'BBL' in the energy dataset to pass it to the 'on' keyword argument of the merge pandas method: pd.merge(..... on=['BBL']) will use the common column 'BBL' to join the information from the 2 datasets for each BBL value (check out the complete syntax!). YOu can also say pd.merge(..., right_on=BBL, left_on=\"NYC Borough, Block, and Lot (BBL)'). Always make sure though that the data type is the same! both integers, both strings, or whatever but the same, or you will not be able to merge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T15:47:20.434273",
     "start_time": "2017-10-19T15:47:20.426426"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(bsize1.BBL.values[0]), (nrg1.BBL.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T15:47:32.313527",
     "start_time": "2017-10-19T15:47:32.267121"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bblnrgdata = pd.merge(nrg1, bsize1, on='BBL').dropna()\n",
    "bblnrgdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bblnrgdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T15:48:56.747639",
     "start_time": "2017-10-19T15:48:52.459258"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# prepare your BBL columns\n",
    "#nrg.rename...\n",
    "#merge\n",
    "#bblnrgdata = pd.merge(...)\n",
    "\n",
    "\n",
    "# Now the scatter matrix plot should show more columns.\n",
    "scatter_matrix (bblnrgdata, s=30, figsize=(16, 16));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T15:49:16.596228",
     "start_time": "2017-10-19T15:49:16.591281"
    }
   },
   "source": [
    "Figure 2: scatter matix of final dataset (please describe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "once you have the dataframe with all the info you want, you want to plot Energy vs Number of Units in the Building.  **Energy TOTAL, not per sq ft...** Here you can choose what you think makes more sense for the number of units: all units, residential units... \n",
    "\n",
    "**Make a  scatter plot first of Energy vs Units. It will look really bad be cause all the datapoints are at very low Unit numbers while the Unit number range actually goes up to 8000. **\n",
    "\n",
    "\n",
    "Make a second plot that zooms into the region where most points are by cutting your x and y axis plotted: e,g, use xlim=(1000,1e10), ylim=(1,1000), where the numbers to be plugged in depend on exactly what you chose to plot\n",
    "\n",
    "I left my plots below as guidance. **Remember, each plot needs a descriptive caption, and axis labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bblnrgdata['Total Energy Consumption (kBtu)'] = bblnrgdata['Reported Property Floor Area (Building(s)) (ft\\xc2\\xb2)']*bblnrgdata['Site EUI(kBtu/ft2)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bblnrgdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# first scatter plot\n",
    "pl.figure(figsize=[20,20])\n",
    "pl.scatter(bblnrgdata['Total Energy Consumption (kBtu)'],bblnrgdata['UnitsTotal'],label='Total Energy Consumption')\n",
    "pl.xlabel('Total Energy Consumption (kBtu)')\n",
    "pl.ylabel('Number of Units')\n",
    "pl.legend()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# zoomed in scatter plot\n",
    "pl.figure(figsize=[20,20])\n",
    "pl.scatter(bblnrgdata['Total Energy Consumption (kBtu)'],bblnrgdata['UnitsTotal'],label='Total Energy Consumption')\n",
    "pl.xlabel('Total Energy Consumption (kBtu)')\n",
    "pl.ylabel('Number of Units')\n",
    "pl.xlim([1000,1e8])\n",
    "pl.ylim([1,1000])\n",
    "pl.legend()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** IMPORTANT NOTE ABOUT LOGS AND LOG PLOTS **\n",
    "in class we talked about logs when we talked about likelihood: often we prefer working with the log(likelihood) instead of the likelihood, and since all problems involving likelihood are about maximization (find the maximum likelihood to find the best fit parameters) and the log is a MONOTONIC function (log(x) grows when x grows, and gets smaller when x gets smaller) the maximum of the likelihood of a model with be in the same place as the maximum of the log(likelihood). \n",
    "\n",
    "Another great thing about logarithm: **when the points in a plot all look scrunched against the axis **\n",
    "\n",
    "\n",
    "**Try to make a log plot instead**. In pandas you enable that with the keyword 'loglog' : bblnrgdata.plot(..... loglog=True)\n",
    "\n",
    "This will compress the high  x and high  y values, and compress the small x and small y values. \n",
    "\n",
    "\n",
    "\n",
    "NOTICE THAT YOU WILL STILL HAVE TO CUT YOUR DATASET! in my data I had a lot of energy datapoints that were exactly 0. I removed these \"outliers\" which I think are truly outliers in the sense that they are misreported numbers. you can remove the data that have nrg==0 (or nrg < some sensible threshold choice) by indexing your array: something like bblnrgdata_cut = bblnrgdata[bblnrgdata.nrg>1000]\n",
    "\n",
    "Also I removed the buildings with several thousand units. points like that at the edge of my range would have a lot of \"LEVERAGE\", however they are not suprious entries like the 0, which i believe are missing values, or perhaps abandoned lots. these are good datapoint that i need to throw away functionally for my analysis to work, but that should be stated clearly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#you may need to change the name of this column under some versions of pandas\n",
    "bblnrgdata['Reported Property Floor Area (Building(s))'] = \\\n",
    "            pd.to_numeric(bblnrgdata['Reported Property Floor Area (Building(s)) (ft²)'], \n",
    "                          errors='coerce').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bblnrgdata.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-19T17:28:47.312217",
     "start_time": "2017-10-19T17:28:45.424304"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#the line below checks that the conversion worked. should be removed in delivery ready ipynb\n",
    "#nrg['Reported Property Floor Area (Building(s)) (ft²)'].astype(float)#log plot\n",
    "bblnrgdata['nrg'] = bblnrgdata['Total Energy Consumption (kBtu)']/bblnrgdata['DOF Number of Buildings']\n",
    "\n",
    "bblnrgdataCut = bblnrgdata[(bblnrgdata.nrg > 1000) * (bblnrgdata.UnitsTotal>=10) * \n",
    "                           (bblnrgdata.UnitsTotal<1000)]\n",
    "\n",
    "ax = bblnrgdataCut.plot(kind='scatter', y='nrg', x='UnitsTotal', \n",
    "                   marker='o',  figsize=(16, 14), loglog=True)\n",
    "yl = ax.set_xlabel(\"Number of Units in Building\", fontsize=20)\n",
    "xl = ax.set_ylabel(\"Energy consumption per building (kBtu)\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Now fit a line through the data. you can use whatever you want to do it: statsmodels, scipy, any other package, or write your own minimization of the residuals\n",
    "\n",
    "## BUT REMEMBER: we see hints of a linear relation in log space! so we want to fit a line to the log of our variables, not the variables themselves:\n",
    "if you used statsmodels it would look something like this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## choose  which is your DEPENDENT and which is your INDEPENDENT variable. \n",
    "which is the \"logical\" IV: what are we assuming depends on what? energy on size of building or building on size of energy... discuss this but also test both fits, energy vs size and size vs energy. how can you compare these models? \n",
    "\n",
    "I WILL UPLOAD SOME MORE INSTRUCTIONS TONIGHT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "\n",
    "\n",
    "1. **Fit a line** to Units vs Energy. The independent variable in this problem should be number of units, but try fit both Unity to energy and energy to unit.\n",
    "2. **Fit a line** to Energy vs Units.\n",
    "3. **Evaluate which is better by calculating the chi square**.  Can you compare these models with the likelihood ratio test? (hint: are they nested??) I provide a function for that or you can write your own. *Assume poisson statistics for the errors on the independent variable*. \n",
    "    The function is \n",
    "    \n",
    "    chisq = $\\sum_i \\frac{(model(x_i) - data(x_i))^2 }{ error_i^2}$\n",
    "    \n",
    "    where the sum is over all datapoints, \n",
    "    \n",
    "    for the i-th value with x value $x_i$ model is the predction of your fit for $x_i$, \n",
    "    \n",
    "    $data(x_i)$ \n",
    "    is your observation, \n",
    "    \n",
    "    and $error_i$ is $\\sqrt{data(x_i)}$\n",
    "    (but remember you worked in log space! What are the proper errors??)\n",
    "    \n",
    "4. **Fit a 2nd degree polynomial** to the Units vs Energy (with statsmodels.formulae.api.ols() for example passing the formula for a parabola, like we did in class. The formula for a 2nd deg polynomial is \n",
    "    $y = ax^2 + bx + c$ .\n",
    "\n",
    "5. **Compare the Units vs Energy line fit and the Units vs Energy 2-nd degree polynomial fit with the Likelihood ratio test**. The formula is:\n",
    "    \n",
    "    LR  =  -2 * (logLikelihood_Model1 - logLikelihood_Model2)\n",
    "    \n",
    "    where Model1 is the least complex (fewer parameters).\n",
    "    \n",
    "    Th logLikelihood can be extracted from the model summary when using statsmodels.\n",
    "    \n",
    "    Compare this LR statistics to a chi sq table (for example http://passel.unl.edu/Image/Namuth-CovertDeana956176274/chi-sqaure%20distribution%20table.PNG) and say if *at alpha = 0.05* Model1 is preferible to Model2. The LR is chi^2 distributed with number of degrees of freedom N_{DOF} = parameters_Model2 - parameters_Model1\n",
    "    \n",
    "    \n",
    "    Also if you used statsmodels for the fit you can use the \n",
    "    compare_lr_test() method of your fit and verify you got the right answer.  Use the method compare_lr_test() of the most complex model of the 2 and pass it the result of stats models for the simpler fit \n",
    "    (e.g. smf.ols(formula = ...).fit().compare_lr_test(sm.OLS(...).fit()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fit a line to Units vs Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# fits and plots here\n",
    "bblnrgdataCut.head()\n",
    "bblnrgdataCut['log_Units'] = np.log10(bblnrgdataCut['UnitsTotal'])\n",
    "bblnrgdataCut['log_Energy'] = np.log10(bblnrgdataCut['Total Energy Consumption (kBtu)'])\n",
    "# your plots should show datapoints (as scatter plot) and models (as lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# my OLS summary. \n",
    "linemodel= smf.ols(formula='log_Energy~log_Units',data=bblnrgdataCut).fit()\n",
    "linemodel.summary()\n",
    "# Yours may be somewhat different depending on how you cut the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize=[16,16])\n",
    "pl.scatter(bblnrgdataCut['log_Units'],bblnrgdataCut['log_Energy'],label='log_Energy')\n",
    "pl.plot(bblnrgdataCut['log_Units'],linemodel.predict(),'r')\n",
    "pl.xlabel('log_Units')\n",
    "pl.ylabel('log_Energy')\n",
    "pl.legend()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fit a line to Energy vs Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linemodel2 = smf.ols(formula='log_Units~log_Energy',data=bblnrgdataCut).fit()\n",
    "linemodel2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize=[16,16])\n",
    "pl.scatter(bblnrgdataCut['log_Energy'],bblnrgdataCut['log_Units'],label='log_Units')\n",
    "pl.plot(bblnrgdataCut['log_Energy'],linemodel2.predict(),'r')\n",
    "pl.xlabel('log_Energy')\n",
    "pl.ylabel('log_Units')\n",
    "pl.legend()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chisquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "errors_energy = np.sqrt((bblnrgdataCut['Reported Property Floor Area (Building(s)) (ft\\xc2\\xb2)'])**2 +\\\n",
    "                (bblnrgdataCut['Site EUI(kBtu/ft2)']**2))\n",
    "errors_units = np.sqrt(bblnrgdataCut.UnitsTotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errorsInLogEnergy = np.abs(errors_energy / bblnrgdataCut['Total Energy Consumption (kBtu)'] / np.log(10))\n",
    "errorsInLogUnits = np.abs(errors_units / bblnrgdataCut.UnitsTotal / np.log(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chisq(x,y,lm,errors):\n",
    "    return np.sum((lm.predict()-y)**2/(errors)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chisq1 = chisq(bblnrgdataCut['log_Units'],bblnrgdataCut['log_Energy'],linemodel,errorsInLogEnergy)\n",
    "chisq2 = chisq(bblnrgdataCut['log_Energy'],bblnrgdataCut['log_Units'],linemodel2,errorsInLogUnits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chisq1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chisq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (chisq1-1) > (chisq2-1):\n",
    "    print('Energy vs Units model is better')\n",
    "else:\n",
    "    print('Units vs Energy model is better')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fit a 2nd degree polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "curvemodel = smf.ols(formula='log_Energy~I(log_Units**2)+log_Units',data=bblnrgdataCut).fit()\n",
    "curvemodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize=[16,16])\n",
    "pl.scatter(bblnrgdataCut['log_Units'],bblnrgdataCut['log_Energy'],label='log_Energy')\n",
    "pl.plot(bblnrgdataCut['log_Units'],curvemodel.predict(),'r.')\n",
    "pl.xlabel('log_Units')\n",
    "pl.ylabel('log_Energy')\n",
    "pl.legend()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare the Units vs Energy line fit and the 2-nd degree polynomial fit with the Likelihood ratio test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print (\"LR : \", -2 * (linemodel.llf - curvemodel.llf))\n",
    "print (\"LR from statsmodels:\", curvemodel.compare_lr_test(linemodel)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if curvemodel.compare_lr_test(linemodel)[0] > 3.84:\n",
    "    print('We could reject our null hypothesis that the curvemodel is not better than the linemodel')\n",
    "else:\n",
    "    print('We could not reject our null hypothesis that the curvemodel is not better than the linemodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if curvemodel.compare_lr_test(linemodel)[1] < 0.05:\n",
    "    print('We could reject our null hypothesis that the curvemodel is not better than the linemodel')\n",
    "else:\n",
    "    print('We could not reject our null hypothesis that the curvemodel is not better than the linemodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Extra credit 1: calculate and plot the likelihood surface\n",
    "Create a function that minimizes the residuals:\n",
    "\n",
    "the residuals are the sum of the differences between data and model: in the case of a line fit model. Use the same function you created for the chi^2 test.\n",
    "\n",
    "You should sum over each datapoints the residuals squared, which should look something like\n",
    "\n",
    "(np.log(bblnrgdatacut.nrg) - np.log(bblnrgdatacut.UnitsTotal)*a+b )^2 / errors^2\n",
    "\n",
    "where a and b are the parameters returned by the line fitter. \n",
    "\n",
    "For each data point you can calculate the model at different values : for example in a range B = np.arange (-100, 100, 1) for the intercept, and A = np.arange(-50.0, 50.0, 0.5) for the slope.\n",
    "\n",
    "\n",
    "You can write it as a nested for loop (or challenge yourself and vectorize it!) with a loop inside another ranging all poissible combinations of the 2 variables (i use enumerate to get both an index from 0 to the size of my array, which i assign to i (and j) and the value of the array at that index - look up the syntax!):\n",
    "\n",
    "\n",
    "Lsurface = np.zeros((len(A), len(B)))\n",
    "for i,a in enumerate(A):\n",
    "    for j,b in enumerate(B):\n",
    "         Lsurface[i][j] = np.nansum(residuals(a,b,data,errors)) .....\n",
    "\n",
    "this gives you a 2D array that represents your likelihood surface! What we do to find a good fit is find the minimum (lowest point) on this surface.\n",
    "You can plot a surface (a 2D array) with pl.imshow(Lsurface) as a \"heatmap\" but when you do that you will find that the plot is very uninformative. just like you did before with the data, plot the log of it (pl.imshow(np.log(Lsurface)). Also make sure your x and y axes tick numbers represent the range of values, not the cell index, which is the default for imshow. Inputting your data in the cell below should give a plot similar to mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residuals(a,b,data,errors):\n",
    "    x = data.x\n",
    "    y = data.y\n",
    "    return np.nansum((x*a+b-y)**2/(errors)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bblnrgdataCut.x = bblnrgdataCut['log_Units']\n",
    "bblnrgdataCut.y = bblnrgdataCut['log_Energy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.arange(-50.0,50.0,0.5)\n",
    "B = np.arange(-100.0,100.0,1)\n",
    "Lsurface = np.zeros((len(A),len(B)))\n",
    "for i,a in enumerate(A):\n",
    "    for j,b in enumerate(B):\n",
    "        Lsurface[i][j] = residuals(a,b,bblnrgdataCut,errorsInLogEnergy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize=(10,10))\n",
    "pl.title (\"log likelihood surface\", fontsize = 22)\n",
    "pl.imshow(np.log(Lsurface), extent = [-50,50,100,-100], aspect=0.5)\n",
    "pl.xlabel('slope', fontsize = 22)\n",
    "pl.ylabel('intercept', fontsize = 22)\n",
    "pl.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## EXTRA CREDIT: get creative with the dataset. can you make an insigntful plot to show any structure in the data?\n",
    "\n",
    "below I am mapping the building age to a colormap and the ratio of total to residential units to the size of the datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bblnrgdata['YearBuilt'][bblnrgdata['YearBuilt']<1800]=1800\n",
    "\n",
    "bblnrgdata.plot(kind='scatter',x='nrg',y='UnitsTotal', \n",
    "                fontsize=22, colormap='gist_rainbow', alpha = 1, \n",
    "                marker='o',  figsize=(16, 14), loglog=True,  \n",
    "                xlim=(1000,1e11), ylim=(1,1000), \n",
    "                c=bblnrgdata['YearBuilt']-1900, \n",
    "                s=bblnrgdata['UnitsTotal']/bblnrgdata['UnitsRes']*100)\n",
    "pl.title('Color maps Age in years, Size maps tital/residential units', fontsize=18)\n",
    "pl.ylabel(\"total number of units\", fontsize=22)\n",
    "pl.xlabel(\"total energy consumption (kBtu)\", fontsize=22)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PUI2016_Python2",
   "language": "python",
   "name": "pui2016_python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "323px",
    "left": "0px",
    "right": "617.333px",
    "top": "130px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
